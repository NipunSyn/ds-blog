{
  
    
        "post0": {
            "title": "Evaluation Metrics for Classification",
            "content": "Why do we need a metric in the first place . In machine learning (supervised learning to be exact), our objective is to create a mathematical model that helps us make predictions based on some prior distribution of independent and dependent variables that we have and update the model weights in a way that we minimize loss. The loss function is a mathematical function that helps the algorithm converge, but for layman interpretation, it is not of much use (most of the time). . This is where evaluation metrics help us as, depending on the choice of the metric, they give us an indication of how our model is performing. It is just like evaluating your performance based on your report card in school or your CG in college to assess how you are performing (acadamically at least) - just not that straightforward xD . Some of the most common evaluation metrics, which we will also discuss in this post, are: accuracy, precision, recall, and F1 score. I will leave a few other important metrics to another blog post as they are a bit more difficult to intuitively understand. . Terminology . Before we delve into how we interpret evaluation metrics, it is important to understand some basic terminology which could look pretty daunting on the surface but is pretty straightforward once you get the hang of it. For example let’s say you are building a simple image classifier that classifies an image as a dog or a cat (I know, I know, very cliché). This is an example of a binary classifier and the following predictions are possible: . True Positive: When your model predicts that an observation belongs to a class, and that observation actually belongs to that class. You predict something as dog, and it actually is a dog | True Negative: When you predict that something does not belong to a class, and it actually does not belong to that class. In binary classification, if you predict something to be in one class, that by definition means it does not belong to the other class. Here, say we predict something as not dog (ie, predict it as cat as there are only two classes) and it actually is not a dog (is a cat). | False Positive: When you predict an observation to a class but it actually does not belong to it. Predicting something as a dog when it was actually a cat. | False Negative: When you predict an observation does not belong to a class but it actually belongs to it. For example, predicting that an observation is not a dog (is a cat) when in fact it is a dog | . Now depending on your project, you might want to prioritise one over the other. In a project like predicting whether a person has Covid or not, you would want to reduce the number of false negatives as much as possible because one wrong prediction could lead to a lot of problems to not only that person, but others around him. Similarly, in a hypothetical project where you want to predict whether a person is a criminal or not you would want to minimise the number of false positives as putting an innocent person behind bars just isn’t the best thing to do xD . Confusion Matrix . Finally, let’s look at something called the confustion matrix- something you are more or less certain to encounter whenever you take up a classification problem. For binary classification, it is a simple two by two matrix that summarises our model’s predictions. It comes in handy in not only identifying the number of errors our model makes, but also the type of errors made by it- whether it is biased to one class. On the left you have actual class labels that we provide to the model, and on top you have the predicted labels. The grid values, thus, provide the number of observations in each possible bucket (the ones discussed above) . Evaluation Metrics . Accuracy . It is the simplest metric to calculate, as well as interpret. It is simply the ratio of the number of predictions that our model made correctly to the total number of predictions made. Mathematically, it is the ratio of the sum of true positives and true negatives to the total number of predictions. On the surface, it might look like a pretty good metric but in practice, accuracy is hardly ever used in the industry. Let’s look at an examle to understand why. . Say on a given day you have 10,000 passengers travelling to work of which 10 are Covid positive. As you can guess, the data is very imbalanced; meaning the number of people who are Covid negative far exceeds the number of people that are Covid positive. Now say, we test our model on this and our model classifies all the passengers as Covid negative. . . This will give us an accuracy of: . accuracy=(tp+tn)/(tp+tn+fp+fn)accuracy = (tp + tn) / (tp + tn + fp + fn)accuracy=(tp+tn)/(tp+tn+fp+fn) accuracy=(0+9,990)/(10,000)=99.9accuracy = (0 + 9,990) / (10,000) = 99.9%accuracy=(0+9,990)/(10,000)=99.9 . As we can see, the model got an accuracy of 99.9%, and it might look very impressive. However, one can easily see that the model is more or less useless as classifying everyone as Covid negative could end up being very detrimental. Accuracy is not the correct metric to use when our dataset is so imbalanced. Imbalanced dataset is one in which the number of observations falling in one class (here, Covid negative) far exceeds the number of observations in the other class. . Recall . To tackle this problem of imbalanced dataset where minimising the number of false negatives is of utmost priority, we use the recall score. In simple terms, recall score gives you the fraction of correctly identified positive observations out of all the positive observations. Notice the denominator is the sum of all the actual positive observations. . recall=tp/(tp+fn)recall = tp/(tp + fn)recall=tp/(tp+fn) recall=0/(0+10)=0recall = 0/(0 + 10) = 0recall=0/(0+10)=0 . As you can see, although our accuracy score was almost perfect, we scored 0 in our recall score. Of all the actual positives, we failed to identify even one! . Precision . Say that now, after looking that our recall is 0, we change our model in such a way that we predict all the people as Covid positive. This would mean that although our accuracy would be close to 0, our recall will be 1 and we won’t make any mistakes in predicting people that are Covid positive. Again, you can very clearly see that our model is fundamentally flawed as predicting everyone as positive would lead to huge medical costs - from screening, to isolating the patients, to providing them with food and other amenities during quarantine. This model, as good as it may be in predicting positives correctly, cannot be used in real life as the cost would be too high. To quantify this, we can look at the precision score of our predictions. Precision gives the fraction of observations correctly identified as positive out of all the observations predicted as positive. . . In this example, our precision would be: precision=tp(tp+tn)precision = tp(tp + tn)precision=tp(tp+tn) precision=10(10+9,990)=0.001precision = 10(10 + 9,990) = 0.001precision=10(10+9,990)=0.001 . This score is pretty bad to say the least xD . F1 Score . For most classification problems- and especially with problems that have an imbalanced class distribution, we will have a tradeoff between precision and recall. We might have cases where both false positives and false negatives are both equally undesirable. In such a case, we would ideally want to maximize both precision and recall but as mentioned above, there will always be a tradeoff between the two as increasing recall decreases precision and vice versa. Thus, it becomes important to reduce the problem to one single metric and this is where F1 score helps us. By definition, F1 score is simply the harmonic mean of precision and recall. . F1=2(precisionrecall)/(precision+recall)F1 = 2 _ (precision _ recall) / (precision + recall)F1=2(​precisionr​ecall)/(precision+recall) . And just like all the other metrics discussed in this post, the range of F1 Score is [0, 1] where a score closer to 1 means the model is performing well. Now you may be wondering why the harmonic mean is used and not arithmetic (or even geometric). The simple reason behhind this is that harmonic mean discourages large difference in precision and recall more than arithmetic and geometric means. For example, let’s say you have a mode A with the follwing metrics: . Precision Recall . 0.7 | 0.8 | . The F1 score for this is 0.746. The arithmetic and geometric means for the same are 0.75 and 0.748 respectively. Now say you have another model B that has the following metrics: . Precision Recall . 0.3 | 0.8 | . As you can see, for this model, the difference in precision and recall values is pretty high. The arithmetic and geometric means for this model are 0.55 and 0.489 respectively. At the same time, the F1 score (or the harmonic mean) for this model is 0.436 which is significantly smaller than both arithmetic mean and geometric mean. This is what is meant by harmonic mean discourages large difference in two values and penalizes them more heavily - making it a better metric. . Thus, for problems where both false negatives and false positives are equally undesirable, F1 score is a very powerful metric. . When to use which metric . The choice of metric greatly impacts the evaluation of your model and could result in it making a lot of impact, or, on the flip side, costing the company a lot of money. As a rule of thumb, if your project requires minimizing the number of false negatives (for example, the Covid prediction model we discussed above), you would want to use recall as your metric. If it is false positives that are undesirable (for example, the hypothetical court case we discussed above), you would want to use precision as your metric. And for projects where both false negatives and false positives are not desirable, F1 score is a very good choice of metric. . It is important to remember that these are only a few of the metrics used in the industry, but at the same time their importance and interpretation cannot be discounted. . That’s it for this post, where we covered the fundamental classificaiton metrics frequently used in the industy and tackled few of the common doubts associated with them. .",
            "url": "https://nipunsyn.github.io/ds-blog/metrics/classification/2022/07/17/classification_metrics.html",
            "relUrl": "/metrics/classification/2022/07/17/classification_metrics.html",
            "date": " • Jul 17, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Evaluation Metrics for Classification",
            "content": "Why do we need a metric in the first place . In machine learning (supervised learning to be exact), our objective is to create a mathematical model that helps us make predictions based on some prior distribution of independent and dependent variables that we have and update the model weights in a way that we minimize loss. The loss function is a mathematical function that helps the algorithm converge, but for layman interpretation, it is not of much use (most of the time). . This is where evaluation metrics help us as, depending on the choice of the metric, they give us an indication of how our model is performing. It is just like evaluating your performance based on your report card in school or your CG in college to assess how you are performing (acadamically at least) - just not that straightforward xD . Some of the most common evaluation metrics, which we will also discuss in this post, are: accuracy, precision, recall, and F1 score. I will leave a few other important metrics to another blog post as they are a bit more difficult to intuitively understand. . Terminology . Before we delve into how we interpret evaluation metrics, it is important to understand some basic terminology which could look pretty daunting on the surface but is pretty straightforward once you get the hang of it. For example let&#39;s say you are building a simple image classifier that classifies an image as a dog or a cat (I know, I know, very cliché). This is an example of a binary classifier and the following predictions are possible: . True Positive: When your model predicts that an observation belongs to a class, and that observation actually belongs to that class. You predict something as dog, and it actually is a dog | True Negative: When you predict that something does not belong to a class, and it actually does not belong to that class. In binary classification, if you predict something to be in one class, that by definition means it does not belong to the other class. Here, say we predict something as not dog (ie, predict it as cat as there are only two classes) and it actually is not a dog (is a cat). | False Positive: When you predict an observation to a class but it actually does not belong to it. Predicting something as a dog when it was actually a cat. | False Negative: When you predict an observation does not belong to a class but it actually belongs to it. For example, predicting that an observation is not a dog (is a cat) when in fact it is a dog | . Now depending on your project, you might want to prioritise one over the other. In a project like predicting whether a person has Covid or not, you would want to reduce the number of false negatives as much as possible because one wrong prediction could lead to a lot of problems to not only that person, but others around him. Similarly, in a hypothetical project where you want to predict whether a person is a criminal or not you would want to minimise the number of false positives as putting an innocent person behind bars just isn&#39;t the best thing to do xD . Confusion Matrix . Finally, let&#39;s look at something called the confustion matrix- something you are more or less certain to encounter whenever you take up a classification problem. For binary classification, it is a simple two by two matrix that summarises our model&#39;s predictions. It comes in handy in not only identifying the number of errors our model makes, but also the type of errors made by it- whether it is biased to one class. On the left you have actual class labels that we provide to the model, and on top you have the predicted labels. The grid values, thus, provide the number of observations in each possible bucket (the ones discussed above) . Evaluation Metrics . Accuracy . It is the simplest metric to calculate, as well as interpret. It is simply the ratio of the number of predictions that our model made correctly to the total number of predictions made. Mathematically, it is the ratio of the sum of true positives and true negatives to the total number of predictions. On the surface, it might look like a pretty good metric but in practice, accuracy is hardly ever used in the industry. Let&#39;s look at an examle to understand why. . Say on a given day you have 10,000 passengers travelling to work of which 10 are Covid positive. As you can guess, the data is very imbalanced; meaning the number of people who are Covid negative far exceeds the number of people that are Covid positive. Now say, we test our model on this and our model classifies all the passengers as Covid negative. . This will give us an accuracy of: . $$ accuracy = (tp + tn) / (tp + tn + fp + fn) $$ $$ accuracy = (0 + 9,990) / (10,000) = 99.9% $$ . As we can see, the model got an accuracy of 99.9%, and it might look very impressive. However, one can easily see that the model is more or less useless as classifying everyone as Covid negative could end up being very detrimental. Accuracy is not the correct metric to use when our dataset is so imbalanced. Imbalanced dataset is one in which the number of observations falling in one class (here, Covid negative) far exceeds the number of observations in the other class. . Recall . To tackle this problem of imbalanced dataset where minimising the number of false negatives is of utmost priority, we use the recall score. In simple terms, recall score gives you the fraction of correctly identified positive observations out of all the positive observations. Notice the denominator is the sum of all the actual positive observations. . $$ recall = tp/(tp + fn) $$ $$ recall = 0/(0 + 10) = 0 $$ . As you can see, although our accuracy score was almost perfect, we scored 0 in our recall score. Of all the actual positives, we failed to identify even one! . Precision . Say that now, after looking that our recall is 0, we change our model in such a way that we predict all the people as Covid positive. This would mean that although our accuracy would be close to 0, our recall will be 1 and we won&#39;t make any mistakes in predicting people that are Covid positive. Again, you can very clearly see that our model is fundamentally flawed as predicting everyone as positive would lead to huge medical costs - from screening, to isolating the patients, to providing them with food and other amenities during quarantine. This model, as good as it may be in predicting positives correctly, cannot be used in real life as the cost would be too high. To quantify this, we can look at the precision score of our predictions. Precision gives the fraction of observations correctly identified as positive out of all the observations predicted as positive. . In this example, our precision would be: $$ precision = tp/(tp + tn) $$ $$ precision = 10(10 + 9,990) = 0.001 $$ . This score is pretty bad to say the least xD . F1 Score . For most classification problems- and especially with problems that have an imbalanced class distribution, we will have a tradeoff between precision and recall. We might have cases where both false positives and false negatives are both equally undesirable. In such a case, we would ideally want to maximize both precision and recall but as mentioned above, there will always be a tradeoff between the two as increasing recall decreases precision and vice versa. Thus, it becomes important to reduce the problem to one single metric and this is where F1 score helps us. By definition, F1 score is simply the harmonic mean of precision and recall. . $$ F1 = 2 * (precision * recall) / (precision + recall) $$ . And just like all the other metrics discussed in this post, the range of F1 Score is [0, 1] where a score closer to 1 means the model is performing well. Now you may be wondering why the harmonic mean is used and not arithmetic (or even geometric). The simple reason behhind this is that harmonic mean discourages large difference in precision and recall more than arithmetic and geometric means. For example, let&#39;s say you have a mode A with the follwing metrics: . Precision Recall . 0.7 | 0.8 | . The F1 score for this is 0.746. The arithmetic and geometric means for the same are 0.75 and 0.748 respectively. Now say you have another model B that has the following metrics: . Precision Recall . 0.3 | 0.8 | . As you can see, for this model, the difference in precision and recall values is pretty high. The arithmetic and geometric means for this model are 0.55 and 0.489 respectively. At the same time, the F1 score (or the harmonic mean) for this model is 0.436 which is significantly smaller than both arithmetic mean and geometric mean. This is what is meant by harmonic mean discourages large difference in two values and penalizes them more heavily - making it a better metric. . Thus, for problems where both false negatives and false positives are equally undesirable, F1 score is a very powerful metric. . When to use which metric . The choice of metric greatly impacts the evaluation of your model and could result in it making a lot of impact, or, on the flip side, costing the company a lot of money. As a rule of thumb, if your project requires minimizing the number of false negatives (for example, the Covid prediction model we discussed above), you would want to use recall as your metric. If it is false positives that are undesirable (for example, the hypothetical court case we discussed above), you would want to use precision as your metric. And for projects where both false negatives and false positives are not desirable, F1 score is a very good choice of metric. . It is important to remember that these are only a few of the metrics used in the industry, but at the same time their importance and interpretation cannot be discounted. . That&#39;s it for this post, where we covered the fundamental classificaiton metrics frequently used in the industy and tackled few of the common doubts associated with them. .",
            "url": "https://nipunsyn.github.io/ds-blog/metrics/classification/2022/07/17/classification-metrics.html",
            "relUrl": "/metrics/classification/2022/07/17/classification-metrics.html",
            "date": " • Jul 17, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Creating a Conda Virtual Environment",
            "content": "What is a virtual environment . Beginning any new project would require you to have a set of tools that would help you perform the required task at hand. Similarly, before starting any Machine Learning project, it is important to set up an isolated environment where you download all the required libraries - for data manipulation, doing math, visualisation, modelling, and so on - that will act as your set of tools for that project. Creating an isolated environment for a new project (or for a set of similar projects) not only helps you avoid any dependency issues that you might typically encounter (for example, missing libraries) but also makes it easy for you to make your code available to others, as all they would need to do is create an environment identical to yours and simply run the code! . Setting up an environment on M1 Macbook . Prerequisites . The first thing you want to do before creating an environment is to ensure that you have XCode installed on your system. XCode contains a collection of tools that are required by your system to build apps, and a lot of Machine Learning packages are also dependent on XCode. Simply go to the Mac App Store and download XCode from there. Do keep in mind that it is huge (~10GB), but you only need to download it once and it is definitely worth the resources. The next thing you want to do is install the XCode command line tools. Simply open up your terminal and paste the following line of code: . xcode-select --install . | Next, you want to download miniforge3 for Apple architecture, and install it to the home directory by pasting the following lines in your terminal: . chmod +x ~/Downloads/Miniforge3-MacOSX-arm64.sh sh ~/Downloads/Miniforge3-MacOSX-arm64.sh source ~/miniforge3/bin/activate . | . Restart your terminal. . Creating a virtual environment . Create and activate a conda environment, install pip: . conda create -n new_env conda activate new_env conda install pip . | If you have a requirements.txt file, simply install it to your environment and you are ready to go (more on this later): . pip install -r requirements.txt . | If you manually want to install packages, you can do that too. For example, you can install numpy like this: . with conda . conda install packagename . | with pip . pip install numpy . | . | Once you are done with the work, simply deactivate your environment: . conda deactivate . | And if you want to delete a virtual environment: . conda env remove -n new_env That&#39;s it. This is the blueprint you need to follow for any project you want to work on. . | . Cloning a repository . Now there may be times when you want to clone a repository and run it locally on your system. This is pretty common in the beginning when all you want to do is run an existing project on your system and simply understand how it functions. Doing that is also pretty easy. Let&#39;s do this using an example repo. . Activate your environment done before . conda activate new_env . | Open up your terminal and create a folder where you want to clone the repo: . mkdir LogReg cd mkdir/ . | Go to the github repository you want to clone and copy the repository link . | . Clone the repository using: . git clone https://github.com/NipunSyn/Logistic-Regression-from-scratch.git . | Go to the folder that contains the requirements.txt file and install it . pip install -r requirements.txt This should install all the required libraries in your environment. That&#39;s it! Simply go to your IDE and select this environment as your kernel and you are ready to go. Not as daunting as it initially felt, right? That&#39;s the case with most of the topics you will encounter on your journey. . | . Hope this post helped you in setting up your environment. In the posts that follow, we will cover a few key topics before moving on to implementing some fundamental models from scratch, and much more. Happy Learning :) . .",
            "url": "https://nipunsyn.github.io/ds-blog/virtual%20environment/git/conda/2022/07/12/create-venv.html",
            "relUrl": "/virtual%20environment/git/conda/2022/07/12/create-venv.html",
            "date": " • Jul 12, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "About me, I’m Nipun Tewari, currently working as an ML Engineer in one of India’s fastest growing fintech startups. I graduated from Birla Institute of Technology and Science in 2022 with a major in Civil Engineering and a minor in Data Science. I started my journey in data science in 2020 when the entire country was under lockdown and all the colleges were shut. So, just like everyone else, I also thought it would be a good idea to learn something new, and now I’m here xD . In my free time, I love reading books especially non-fiction. I was a member of my school, as well as my college football team and never miss a chance to hit the field whenever I get one, and I support Manchester United in the English Premier League. From music, TV series, anime, you can hit me up for anything for a chat. And oh, I also love playing the guitar xD . That’s about me. You can follow me on Twitter: @NipunSyn to connect .",
          "url": "https://nipunsyn.github.io/ds-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nipunsyn.github.io/ds-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}