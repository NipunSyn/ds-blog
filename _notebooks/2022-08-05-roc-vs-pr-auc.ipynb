{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric for Imbalanced Dataset: ROC-AUC vs PR-AUC\n",
    "> In this post we will look at one of the most commonly asked questions, especially in the industry- which metric to use for imbalanced dataset: Receiver Operating Characteristic (ROC) or Precison-Recall Area (PR) Under the Curve\n",
    "- categories: [metrics, classification, imbalanced-data]\n",
    "- badges: true\n",
    "- comments: true\n",
    "- search_exclude: false\n",
    "- image: images/b3_roc-vs-pr/post.jpg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at the metrics first\n",
    "Before we get to the part where we make comparisons, it is always a good idea to first understand both the metrics at an individual level. On a side note, this post is a follow up on the [previous post](https://nipunsyn.github.io/ds-blog/metrics/classification/2022/07/17/classification-metrics.html#Why-do-we-need-a-metric-in-the-first-place) where we discussed some of the standard evaluation metrics for classification. So if you haven't checked that out yet, make sure to go through it first! Plus, for this post you'll need a refresher on TP, FP, TN, FN and the confusion matrix which I have covered in the last post. The summary on [Wikipedia](https://en.wikipedia.org/wiki/Precision_and_recall) is also a very good refresher if you need one. This post will not go into the depths of what ROC-AUC and PR-AUC are, there are quite a few good resources out there explaining the same, I'll link down my go-to resources at the end, so you can give those a go as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC-AUC\n",
    "ROC curve is a plot with True Positive Rate (TPR) on the y axis, measured against False Positive Rate (FPR) on the x axis.\n",
    "- **True Positive Rate** is nothing but recall. Intuitivively, it tells you the fraction of positives out of all the positives your model got right, and can be calculated as $$ TP/(TP + FN) $$\n",
    "- **False Positive Rate** gives you the meaure of the fraction of negatives your classifier marked as positives. Subsequently, it is defined as $$ FP/(TN + FP) $$\n",
    "\n",
    "So the ROC curve plots the TPR and FPR at different thresholds (threshold here being the probability at which you want to classify a sample as positive)- for example, you could use a logistic regression classifier at different thresholds (not just the default 0.5), get the different TPR and FPR values and plot the same. But this method would be highly inefficient. In comes AUC, or area under the curve- a sorting based algorithm that helps tackle this problem and summarises the performance of your classifier in a single number. Also, keep in mind that the ROC curve would want to be at the top left of the graph, as this would maximize TPR, and minimize FPR. (ADD IMAGE)\n",
    "\n",
    "In layman's terms, ROC-AUC is the probability that the classifier will give a higher score (better rank) to a positive sample as opposed to a negative sample (remember, a binary classifier outputs the probability for a sample to be classified as positive). Thus, for an ideal classifier, if you sort all the samples based on probability scores, all the positive samples should lie to the right of all the negative samples. [Two important reasons](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) for which ROC-AUC is preferred are: \n",
    "\n",
    "- AUC is scale-invariant. It measures how well predictions are ranked, rather than their absolute values.\n",
    "- AUC is classification-threshold-invariant. It measures the quality of the model's predictions irrespective of what classification threshold is chosen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PR-AUC\n",
    "Since, by now, we already know what precision and recall are, understanding what precision-recall curve is, and subsequently what PR-AUC is, shouldn't be a difficult task. As opposed to ROC curve, which plots recall (TPR) against FPR, PR curve plots it **against precision** (of all the labels predicted as positive, how many were actually true positives). And keep this definition in mind, as this will prove to be the difference maker in choice between the two evaluation metrics. So how do you plot a PR curve? Again the same steps, compare the probability for a sample being a positive against different thresholds, and for each of these thresholds, calculate the precision and recall (*precision and recall are threshold dependent values* and will vary based on the choice of your threshold). Once you have these different precision and recall values, plot them on a graph with precision on the y axis and recall on the x axis. (ADD IMAGE)\n",
    "\n",
    "Just to recall, precision is given by $$ TP/(TP + FP) $$\n",
    "\n",
    "So how do we interpret how well the model is doing? We use the area under the PR-curve, called PR-AUC (or **average precision**). Again in layman's terms, we can think of PR AUC as the average of precision scores calculated for each recall threshold. As opposed to the ROC curve, the ideal PR curve wants to be at the top right of the graph, which would lead to a recall and precision value of 1. This would imply that we were able to capture all the true positives from the available true positives (recall), and of the samples we said are true positives, all were true positives (precision). Sigh, if only life were this easy!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use PR-AUC for Imbalanced Dataset?\n",
    "Now that we have brushed up on what ROC-AUC and PR-AUC is, let's answer the question that troubles literally everyone in the industry. Which metric to use for imbalanced classification, *especially when the positive class is of more importance.*\n",
    "Okay, let's revisit the definitions of the two. Recall is obviously common in both, it's the second axis that makes all the difference. ROC curve looks at false positive rate: of all the negatives, how many did the model wrongly classify as a positive. PR curve looks at precision: of all the samples predicted as positive by the model, how many were actually positive. Keeping these definitions in mind, let's add the touch we get from an imbalanced dataset.\n",
    "\n",
    "So how does PR Curve help us out? Let's look at the definitions of both.\n",
    "$$ precision = TP/(TP + FP) $$\n",
    "$$ FPR = FP/(TN + FP) $$\n",
    "\n",
    "Now, for a given number of positive and negative samples (the actual dataset), if the number of negative samples is much higher than the positive samples, the denominator will overwhelm the change in numerator in FPR. Simply put, even if we were to get say 10 samples misclassified as FP instead of 5, our FPR will not change drastically as the denominator already contains a huge number of true negatives. This will not let us gauge the performance of the classifier in getting correct positive predictions, as our precision is massively hit if we misclassify 10 samples instead of 5."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example to make things more clear\n",
    "That was a lot of talk, but without actually looking at some numbers it is really difficult to make sense out of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53) \n[Clang 6.0 (clang-600.0.57)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
